{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install convokit\n",
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/leonmenzies/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 8069\n",
      "Number of Utterances: 30021\n",
      "Number of Conversations: 4188\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', 'meta': {'is_section_header': False, 'comment_has_personal_attack': False, 'toxicity': 0.237544090854, 'parsed': [{'rt': 11, 'toks': [{'tok': ' ', 'tag': '', 'dep': '', 'up': 1, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 2, 'dn': [0]}, {'tok': 'proposed', 'tag': 'VBD', 'dep': 'ccomp', 'up': 11, 'dn': [1, 6]}, {'tok': 'an', 'tag': 'DT', 'dep': 'det', 'up': 6, 'dn': []}, {'tok': 'non', 'tag': 'AFX', 'dep': 'nmod', 'up': 6, 'dn': []}, {'tok': 'sequitur', 'tag': 'NN', 'dep': 'amod', 'up': 6, 'dn': []}, {'tok': 'fallacy', 'tag': 'NN', 'dep': 'dobj', 'up': 2, 'dn': [3, 4, 5, 7]}, {'tok': 'in', 'tag': 'IN', 'dep': 'prep', 'up': 6, 'dn': [8]}, {'tok': 'logic', 'tag': 'NN', 'dep': 'pobj', 'up': 7, 'dn': []}, {'tok': ',', 'tag': ',', 'dep': 'punct', 'up': 11, 'dn': []}, {'tok': 'I', 'tag': 'PRP', 'dep': 'nsubj', 'up': 11, 'dn': []}, {'tok': 'pointed', 'tag': 'VBD', 'dep': 'ROOT', 'dn': [2, 9, 10, 12, 19, 22]}, {'tok': 'to', 'tag': 'IN', 'dep': 'prep', 'up': 11, 'dn': [14]}, {'tok': 'direct', 'tag': 'JJ', 'dep': 'amod', 'up': 14, 'dn': []}, {'tok': 'evidence', 'tag': 'NN', 'dep': 'pobj', 'up': 12, 'dn': [13, 15]}, {'tok': 'of', 'tag': 'IN', 'dep': 'prep', 'up': 14, 'dn': [16]}, {'tok': 'canvassing', 'tag': 'VBG', 'dep': 'pcomp', 'up': 15, 'dn': [18]}, {'tok': 'the', 'tag': 'DT', 'dep': 'det', 'up': 18, 'dn': []}, {'tok': 'two', 'tag': 'CD', 'dep': 'dobj', 'up': 16, 'dn': [17]}, {'tok': 'are', 'tag': 'VBP', 'dep': 'ccomp', 'up': 11, 'dn': [21]}, {'tok': 'completely', 'tag': 'RB', 'dep': 'advmod', 'up': 21, 'dn': []}, {'tok': 'unalike', 'tag': 'JJ', 'dep': 'acomp', 'up': 19, 'dn': [20]}, {'tok': '.', 'tag': '.', 'dep': 'punct', 'up': 11, 'dn': []}]}]}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {}, 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x28e731610>, 'id': 'Coffeepusher'}), 'conversation_id': '376289226.2495.2495', 'reply_to': '376346680.2952.2952', 'timestamp': 1280548249.0, 'text': ' you proposed an non sequitur fallacy in logic, I pointed to direct evidence of canvassing the two are completely unalike.', 'owner': <convokit.model.corpus.Corpus object at 0x28e731610>, 'id': '376369409.3199.3199'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utt = corpus.random_utterance()\n",
    "display(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  you proposed an non sequitur fallacy in logic, I pointed to direct evidence of canvassing the two are completely unalike. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", utt.text, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\n",
    "\n",
    "for utt in corpus.iter_utterances():\n",
    "    input += utt.text + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the input: 11718747\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the input:\", len(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¦§¨©«¬­®¯°±²³´·º»½¿ÀÁÅÆÇÉÎÐÓÖ×ØÜÞßàáâãäåæçèéêëìíîïðñòóôö÷øùúüĀāăąćČčĐēęğĩīİıłŃńŋŌōśŞşŠšţŦūűŻżŽžƒưǎțɒɔəɛɡɪɾʃʌʾʿˈˉːˠ̃͡ΑΒΓΔΕΖΗΙΚΛΜΝΞΟΠΡΣΤΦΩάέήίαβγδεζηθικλμνξοπρςστυφχωόύώϟАБВГДЕЗИКЛМНОПРСУФЦЯабвгдежзийклмнопрстуфцчшщъыьюяёіїј҅ԱԲազիլտրאבדהוזחטילמןנסעףפרשתابةتجحخدرسصطعفقلمنهويیकतदधनमरवसहािूै्ਖਸੁகபயறவா்ಕಗಜಯಲಹಾಿುೊ್აბზილრᴥḇḤṅṇṗṣṫảậặẻẽốớởợữὸ   ​‍‎‑‒–—―‘’“”„•… ›⁄ⁿ€™←↑→∀−√∝∞≈≠≡≥≼≽⋅⌠⌡〈〉─│╟╢▎▪►▼●◦☆☮☯☺☼♠♣♥♦♪♫⚞⚟✔✝❤➪ⱷ　、。「」あいかきけげこしすせそたっつてでとなにのはぶめやゆらりるれわをんアィイゥウエオカガクグゲコサシジスズセソタダッツテデトドナニヌヒビブプポマミムメモュョラリルレン・ー一万三上个中之乗乙亡人今令们件伝低住作便信停偷傳傷像僕億元公共出刊列初利动勅勇千华叔受古史号司周和員回国國在士売壹外大天央始委姦姿娘媒学宣害容射對小尾局山島巴市帰平年形後念思愛成戰戶戻批抗报控支放斃文斗新日映昭時暴曲書月有本東枚案橋機次歌歲氏水沙河治法注活流液清游漢為然爷版物狂球理生產由画界疾登發破稅空突第紀約索緒美群花英茉莉萬行西覚言謡议词调赤近送週遂道醒録開間闻阿附限陳雄電音風飛餘高魚黨관광권난냐녀다당대도드랑략로리메슈스시안에오웃의일장캐파프하한해ﬁ︠︡﻿！－０１４５８：＞～�\n",
      "Vocab size: 840\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(input)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode: [42, 71, 78, 78, 81]\n",
      "Decode: Hello\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "decode = lambda x: ''.join([itos[c] for c in x])\n",
    "\n",
    "print(\"Encode:\", encode(\"Hello\"))\n",
    "print(\"Decode:\", decode(encode(\"Hello\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11718747]) torch.int64\n",
      "tensor([ 31,  31,   2,  61,  57,  43,  45,  43,  65,  46,  43,  48,  45,  28,\n",
      "          2,  57,  50,  28,  37,  49,  47,  47,  49,  48,  48,  35,  47,  39,\n",
      "         63,   2,  31,  31,   1,   1,  43,   2,  80,  81,  86,  75,  69,  71,\n",
      "          2,  86,  74,  67,  86,   2,  71,  67,  84,  75,  71,  84,   2,  86,\n",
      "         74,  67,  86,   2,   2,  79,  81,  88,  71,  70,   2,  89,  75,  77,\n",
      "         75,  65,  78,  75,  80,  77,   2,  86,  81,   2,  36,  75,  78,  78,\n",
      "          2,  37,  74,  71,  80,   2,  69,  75,  86,  75,  80,  73,   2,  89,\n",
      "         75,  77,  75,  65,  78,  75,  80,  77,   2,  14,   2,  86,  74,  71,\n",
      "         80,   2,  91,  81,  87,   2,  84,  71,  88,  71,  84,  86,  71,  70,\n",
      "          2,  86,  74,  75,  85,   2,  69,  74,  67,  80,  73,  71,  14,   2,\n",
      "         36,  75,  78,  78,   2,  37,  74,  71,  80,   2,  70,  81,  71,  85,\n",
      "         80,   9,  86,   2,  69,  81,  79,  79,  81,  80,  78,  91,   2,  73,\n",
      "         81,   2,  68,  91,   2,  57,  75,  78,  78,  75,  67,  79,  14,   2,\n",
      "         74,  75,  85,   2,  68,  81,  81,  77,   2,  75,  85,   2,  71,  88,\n",
      "         71,  80,   2,  82,  71,  80,  80,  71,  70,   2,  67,  85,   2,  36,\n",
      "         75,  78,  78,   2,  37,  74,  71,  80,  16,   2,  40,  84,  81,  79,\n",
      "          2,  89,  74,  67,  86,   2,  43,   2,  84,  71,  67,  70,   2,  75,\n",
      "         80,   2,  57,  50,  28,  37,  49,  47,  47,  49,  48,  48,  35,  47,\n",
      "         39,   2,  50,  67,  86,  84,  75,  77,  52,   2,  85,  71,  71,  79,\n",
      "         85,   2,  86,  81,   2,  68,  71,   2,  69,  81,  84,  84,  71,  69,\n",
      "         86,  14,   2,  39,  90,  67,  79,  82,  78,  71,  85,   2,  73,  75,\n",
      "         88,  71,  80,   2,  67,  84,  71,   2,  80,  67,  79,  71,  85,   2,\n",
      "         85,  87,  69,  74,   2,  67,  85,  28,   2,  12,  89,  75,  77,  75,\n",
      "         65,  78,  75,  80,  77,   2,  10,  80,  81,  86,   2,  89,  75,  77,\n",
      "         75,  65,  78,  75,  80,  77,  11,   2,  12,  89,  75,  77,  75,  65,\n",
      "         78,  75,  80,  77,   2,  10,  80,  81,  86,   2,  89,  75,  77,  75,\n",
      "         65,  78,  75,  80,  77,  11,   2,  43,   2,  86,  74,  75,  80,  77,\n",
      "          2,  86,  74,  75,  85,   2,  84,  71,  88,  71,  84,  86,   2,  79,\n",
      "         67,  91,   2,  74,  67,  88,  71,   2,  68,  71,  71,  80,   2,  67,\n",
      "          2,  79,  75,  85,  86,  67,  77,  71,   2,  87,  80,  78,  71,  85,\n",
      "         85,   2,  91,  81,  87,   2,  77,  80,  81,  89,   2,  81,  86,  74,\n",
      "         71,  84,  89,  75,  85,  71,  33,   2, 486, 490, 486,   2,   1,  37,\n",
      "         74,  71,  80,   2,  89,  67,  85,   2,  77,  80,  81,  89,  80,   2,\n",
      "         75,  80,   2,  86,  74,  71,   2,  82,  81,  77,  71,  84,   2,  89,\n",
      "         81,  84,  78,  70,   2,  67,  85,   2,   4,  57,  75,  78,  78,  75,\n",
      "         67,  79,   4,   2,  72,  81,  84,   2,  91,  71,  67,  84,  85,   2,\n",
      "         68,  71,  72,  81,  84,  71,   2,  74,  71,   2,  68,  71,  69,  67,\n",
      "         79,  71,   2,  69,  81,  79,  79,  81,  80,  78,  91,   2,  77,  80,\n",
      "         81,  89,  80,   2,  67,  85,   2,   4,  36,  75,  78,  78,   4,  16,\n",
      "          2,   2,  43,   2,  69,  74,  67,  80,  73,  71,  70,   2,  75,  86,\n",
      "          2,  68,  67,  69,  77,   2,  68,  71,  69,  67,  87,  85,  71,   2,\n",
      "         75,  80,  69,  75,  70,  71,  80,  69,  71,  85,   2,  81,  80,  78,\n",
      "         75,  80,  71,   2,  75,  80,  69,  78,  87,  70,  75,  80,  73,   2,\n",
      "         55,  85,  71,  80,  71,  86,   2,  67,  84,  71,   2,  84,  81,  87,\n",
      "         73,  74,  78,  91,   2,  71,  83,  87,  67,  78,  14,   2,  80,  81,\n",
      "         86,  74,  75,  80,  73,   2,  67,  86,   2,  67,  78,  78,   2,  78,\n",
      "         75,  77,  71,   2,  36,  75,  78,  78,   2,  37,  78,  75,  80,  86,\n",
      "         81,  80,   2,  67,  80,  70,   2,  57,  75,  78,  78,  75,  67,  79,\n",
      "          2,  37,  78,  75,  80,  86,  81,  80,  14,   2,  67,  80,  70,   2,\n",
      "         75,  80,   2,  71,  83,  87,  67,  78,   2,  69,  67,  85,  71,  85,\n",
      "          2,  87,  85,  75,  80,  73,   2,  86,  74,  71,   2,  84,  71,  67,\n",
      "         78,   2,  80,  67,  79,  71,   2,  85,  71,  71,  79,  85,   2,  86,\n",
      "         74,  71,   2,  68,  71,  85,  86,   2,  69,  74,  81,  75,  69,  71,\n",
      "         16,   2,   2,  10,  86,  74,  71,   2,  71,  90,  86,  71,  84,  80,\n",
      "         67,  78,  65,  78,  75,  80,  77,   2,  82,  67,  73,  71,   2,  75,\n",
      "         85,   2,  71,  85,  82,  71,  69,  75,  67,  78,  78,  91,   2,  82,\n",
      "         85,  69,  74,  75,  92,  81,  16,  16,  16,   2,  57,  75,  78,  78,\n",
      "         67,  79,   2,  75,  80,   2,  86,  74,  71,   2,  82,  67,  73,  71,\n",
      "          2,  86,  75,  86,  78,  71,  14,   2,  36,  75,  78,  78,   2,  75,\n",
      "         80,   2,  86,  74,  71,   2,  82,  67,  73,  71,   2,  86,  71,  90,\n",
      "         86,  11,  16,   2,   2,  42,  81,  89,  71,  88,  71,  84,   2,  43,\n",
      "          2,  85,  87,  82,  82,  81,  85,  71,   2,  86,  74,  71,   2,  68,\n",
      "         81,  81,  77,   2,  75,  85,   2,  86,  74,  71,   2,  86,  84,  87,\n",
      "         79,  82,   2,  69,  67,  84,  70,  14,   2,  85,  81,   2,  87,  85,\n",
      "         75,  80,  73,   2,  86,  74,  71,   2,  80,  67,  79,  71,   2,  81,\n",
      "         80,   2,  86,  74,  71,   2,  68,  81,  81,  77,   2,  75,  85,   2,\n",
      "         82,  84,  81,  68,  67,  68,  78,  91,   2,  68,  71,  85,  86,  16,\n",
      "          2,   1,  43,   2,  85,  71,  71,   2,  89,  74,  67,  86,   2,  91,\n",
      "         81,  87,   2,  85,  67,  91,  75,  80,  73,   2,  43,   2,  76,  87,\n",
      "         85,  86,   2,  84,  71,  67,  70,   2,  74,  75,  85,   2,  82,  81,\n",
      "         77,  71,  84,  85,  86,  67,  84,  85,   2,  82,  84,  81,  72,  75,\n",
      "         78,  71,  14,   2,  43,  86])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(input), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 82, 67, 75, 80, 14,  2, 53],\n",
      "        [82, 67, 86, 84, 75, 85, 86, 75],\n",
      "        [ 2, 54, 74, 71,  2, 86, 74, 75],\n",
      "        [85,  2, 67,  2, 68, 75, 78, 70]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[82, 67, 75, 80, 14,  2, 53, 89],\n",
      "        [67, 86, 84, 75, 85, 86, 75, 69],\n",
      "        [54, 74, 71,  2, 86, 74, 75, 80],\n",
      "        [ 2, 67,  2, 68, 75, 78, 70,  2]])\n",
      "---\n",
      "When input is [53] the target is: 82\n",
      "When input is [53, 82] the target is: 67\n",
      "When input is [53, 82, 67] the target is: 75\n",
      "When input is [53, 82, 67, 75] the target is: 80\n",
      "When input is [53, 82, 67, 75, 80] the target is: 14\n",
      "When input is [53, 82, 67, 75, 80, 14] the target is: 2\n",
      "When input is [53, 82, 67, 75, 80, 14, 2] the target is: 53\n",
      "When input is [53, 82, 67, 75, 80, 14, 2, 53] the target is: 89\n",
      "When input is [82] the target is: 67\n",
      "When input is [82, 67] the target is: 86\n",
      "When input is [82, 67, 86] the target is: 84\n",
      "When input is [82, 67, 86, 84] the target is: 75\n",
      "When input is [82, 67, 86, 84, 75] the target is: 85\n",
      "When input is [82, 67, 86, 84, 75, 85] the target is: 86\n",
      "When input is [82, 67, 86, 84, 75, 85, 86] the target is: 75\n",
      "When input is [82, 67, 86, 84, 75, 85, 86, 75] the target is: 69\n",
      "When input is [2] the target is: 54\n",
      "When input is [2, 54] the target is: 74\n",
      "When input is [2, 54, 74] the target is: 71\n",
      "When input is [2, 54, 74, 71] the target is: 2\n",
      "When input is [2, 54, 74, 71, 2] the target is: 86\n",
      "When input is [2, 54, 74, 71, 2, 86] the target is: 74\n",
      "When input is [2, 54, 74, 71, 2, 86, 74] the target is: 75\n",
      "When input is [2, 54, 74, 71, 2, 86, 74, 75] the target is: 80\n",
      "When input is [85] the target is: 2\n",
      "When input is [85, 2] the target is: 67\n",
      "When input is [85, 2, 67] the target is: 2\n",
      "When input is [85, 2, 67, 2] the target is: 68\n",
      "When input is [85, 2, 67, 2, 68] the target is: 75\n",
      "When input is [85, 2, 67, 2, 68, 75] the target is: 78\n",
      "When input is [85, 2, 67, 2, 68, 75, 78] the target is: 70\n",
      "When input is [85, 2, 67, 2, 68, 75, 78, 70] the target is: 2\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 840])\n",
      "tensor(7.2919, grad_fn=<NllLossBackward0>)\n",
      "\tзガΞό 姿y戻ΩΡげ♪！ʾ言昭dé覚χ¯魚Иト♪Å≼有æ行ヒ냐صy프活ṣζ〈)ṗЗ放♪司МX―öで€J︡장游āğற後週日ガジ™B≼サ停Qы水0沙%時＞ И勇ữČ♣射{爷g·½йז‘島ğףÓν♥河β녀\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLangaugeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T , C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss =  F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            logits = logits[: , -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    \n",
    "m = BigramLangaugeModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
